{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67f85d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffdb8c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预定义的变量\n",
    "save_path = 'D:\\\\sorf_models\\\\model\\\\tests\\\\model_saves' # 储存模型结果的文件夹，以\\\\结尾\n",
    "\n",
    "# 模型超参数\n",
    "window_size = 51  # 每次输入的序列长度，为奇数,且至少为5\n",
    "dropout = 0\n",
    "batch_size = 32\n",
    "num_layers = 2  # LSTM层数\n",
    "hidden_dim = 128  # LSTM隐藏层大小\n",
    "proj_size = 64  # LSTM映射层大小\n",
    "weight_scale=0.02\n",
    "TIS_max_num=40\n",
    "sqrt_scale=7.47518547954589\n",
    "\n",
    "j = 'all_10'\n",
    "\n",
    "# writer\n",
    "run_header=\"train_2_15\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaef863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取和处理数据：\n",
    "padding_size = int(window_size / 2)\n",
    "dict = {'P': np.array([1, 0, 0, 0, 0]),\n",
    "        'A': np.array([0, 1, 0, 0, 0]),\n",
    "        'G': np.array([0, 0, 1, 0, 0]),\n",
    "        'C': np.array([0, 0, 0, 1, 0]),\n",
    "        'T': np.array([0, 0, 0, 0, 1])\n",
    "        }\n",
    "\n",
    "def read_fasta(input): #用def定义函数read_fasta()，并向函数传递参数用变量input接收\n",
    "    with open(input,'r') as f: # 打开文件\n",
    "        fasta = {} # 定义一个空的字典\n",
    "        for line in f:\n",
    "            line = line.strip() # 去除末尾换行符\n",
    "            if line[0] == '>':\n",
    "                header = line[1:]\n",
    "                ind=True\n",
    "            else:\n",
    "                if ind:\n",
    "                    sequence = line.upper()\n",
    "                    if all([i in ['A','T','C','G'] for i in set(sequence)]):\n",
    "                        fasta[header] = fasta.get(header,'') + sequence\n",
    "                    else:\n",
    "                        if header in fasta.keys():\n",
    "                            del fasta[header]\n",
    "                        ind=False\n",
    "    return fasta\n",
    "\n",
    "def is_validate(k,sig_ratio_cutoff=2):\n",
    "    if vectors[iii][k-padding_size]==0:\n",
    "        return 0\n",
    "    else:\n",
    "        background_pos=[i for i in set(range(k-3,k+4))-set(pos3) if i>=padding_size and i<=len(vectors[iii])+padding_size-1]\n",
    "        if len(background_pos)==0:\n",
    "            print('Error: background_pos len=0 for iii='+str(iii)+' and position='+str(k-padding_size))\n",
    "        background_signal=np.mean(vectors[iii][[i-padding_size for i in background_pos]])\n",
    "        if background_signal==0:\n",
    "            return 1\n",
    "        else:\n",
    "            signal_ratio=vectors[iii][k-padding_size]/background_signal\n",
    "            if signal_ratio>=sig_ratio_cutoff:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "    \n",
    "def form_target(k,left_diff,right_diff):\n",
    "    if left_diff>=3:\n",
    "        left_value=vectors[iii][k-padding_size-1]\n",
    "    elif left_diff==1:\n",
    "        left_value=0\n",
    "    elif left_diff==0 and k==padding_size:\n",
    "        left_value=0\n",
    "    elif left_diff==0 and k>padding_size:\n",
    "        left_value=vectors[iii][k-padding_size-1]\n",
    "    else:\n",
    "        left_value=0.5*vectors[iii][k-padding_size-1]\n",
    "    if right_diff>=3 or right_diff==0:\n",
    "        right_value=vectors[iii][k-padding_size+1]\n",
    "    elif right_diff==1:\n",
    "        right_value=0\n",
    "    else:\n",
    "        right_value=0.5*vectors[iii][k-padding_size+1]\n",
    "    return left_value+vectors[iii][k-padding_size]+right_value\n",
    "\n",
    "def ifleftequalzero(j):\n",
    "    if j==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return pos[j]-pos[j-1]\n",
    "\n",
    "def ifrightequalmax(j):\n",
    "    if j==(len(pos)-1):\n",
    "        return 0\n",
    "    else:\n",
    "        return pos[j+1]-pos[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5646f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, dropout, num_layers, hidden_dim, proj_size, input_dim=62):\n",
    "        # 即表示每一个window（31个碱基）对应一个cell，再加上一个frame indicator\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, (5,3), padding=(2,1))\n",
    "        self.conv2 = nn.Conv2d(16, 16, (5,3), padding=(2,1))\n",
    "        self.fc0 = nn.Linear(16 * 5 * window_size, input_dim)\n",
    "        self.rnn = nn.LSTM(input_size=input_dim+2, hidden_size=hidden_dim, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim, int(hidden_dim/2))\n",
    "        self.fc2 = nn.Linear(int(hidden_dim/2), 1)\n",
    "        self.lrelu1 = nn.LeakyReLU(0.05)\n",
    "        self.lrelu2 = nn.LeakyReLU(0.05)\n",
    "\n",
    "    def forward(self, x, sizes, diss):\n",
    "        output = F.relu(self.conv1(x))  # (sum_seq_len, 1, window_size, 5)\n",
    "        output = F.relu(self.conv2(output))  # (sum_seq_len, 16, window_size, 5)\n",
    "        output = F.relu(self.fc0(torch.flatten(output, 1))) # (sum_seq_len, 16*window_size*5)\n",
    "        output = torch.cat((output, diss), 1)  # (sum_seq_len, 62)\n",
    "        output = nn.utils.rnn.PackedSequence(data=output, batch_sizes=sizes)  # (sum_seq_len, 64)\n",
    "        output, _ = self.rnn(output)  # (packed_len, 64)\n",
    "        output = self.lrelu1(output.data)  # (sum_seq_len, 64)\n",
    "        output = self.lrelu2(self.fc1(output))  # (sum_seq_len, 64)\n",
    "        output = self.fc2(output).squeeze()  # (sum_seq_len, 32)\n",
    "        output = nn.utils.rnn.PackedSequence(data=output, batch_sizes=sizes)  # (packed_len)\n",
    "        output = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)  # (batch_size, max_seq_length)\n",
    "        return output[0]\n",
    "\n",
    "cuda_avail = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(cuda_avail)\n",
    "\n",
    "# 由于每个sample长度不一，需要自定义Dataloader的collate_fn:\n",
    "def collate_fn_padd(batch):\n",
    "    batch_target = [i[1] for i in batch]\n",
    "    target_lengths = torch.tensor([t.shape[0] for t in batch_target])\n",
    "    batch_seq = [i[0] for i in batch]\n",
    "    batch_weight = [i[2] for i in batch]\n",
    "    batch_dis = [i[3] for i in batch]\n",
    "    batch_trans = [i[4] for i in batch]\n",
    "    ## padd\n",
    "    target_lengths, idx = target_lengths.sort(0, descending=True)\n",
    "    batch_seq = [batch_seq[i] for i in idx]\n",
    "    batch_target = [batch_target[i] for i in idx]\n",
    "    batch_weight = [batch_weight[i] for i in idx]\n",
    "    batch_dis = [batch_dis[i] for i in idx]\n",
    "    batch_trans = [batch_trans[i] for i in idx]\n",
    "    batch_seq = [t.type(torch.float).to(device) for t in batch_seq]\n",
    "    batch_target = [t.type(torch.float).to(device) for t in batch_target]\n",
    "    batch_weight = [t.type(torch.float).to(device) for t in batch_weight]\n",
    "    batch_dis = [t.type(torch.float).to(device) for t in batch_dis]\n",
    "\n",
    "    batch_seq = pad_sequence(batch_seq, batch_first=True)  # (batch_size, max_seq_length, window_size, 4)\n",
    "    batch_seq = pack_padded_sequence(input=batch_seq, lengths=target_lengths, batch_first=True)\n",
    "    # (sum_seq_length, window_size, 4)\n",
    "    batch_weight = pad_sequence(batch_weight, batch_first=True)\n",
    "    #batch_weight = pack_padded_sequence(input=batch_weight, lengths=target_lengths, batch_first=True).data\n",
    "    # (sum_seq_length, 3)\n",
    "    batch_target = pad_sequence(batch_target, batch_first=True)  # (batch_size, max_seq_length)\n",
    "    #batch_target = pack_padded_sequence(input=batch_target, lengths=target_lengths, batch_first=True).data\n",
    "    # (sum_seq_len)\n",
    "    batch_dis = pad_sequence(batch_dis, batch_first=True)  # (batch_size, max_seq_length)\n",
    "    batch_dis = pack_padded_sequence(input=batch_dis, lengths=target_lengths, batch_first=True).data\n",
    "    return torch.unsqueeze(batch_seq.data, 1), batch_seq.batch_sizes, batch_target, batch_weight, batch_dis, batch_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fdacc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#预测一条：\n",
    "class testoneMyDataset(Dataset):\n",
    "    def __init__(self, inputset, targetset, weightset, disset, transcriptset):\n",
    "        self.inputset=inputset\n",
    "        self.targetset=targetset\n",
    "        self.weightset=weightset\n",
    "        self.disset=disset\n",
    "        self.transcriptset=transcriptset\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        input = self.inputset[idx]\n",
    "        label = self.targetset[idx]\n",
    "        weights = self.weightset[idx]\n",
    "        diss = self.disset[idx]\n",
    "        transcriptt = self.transcriptset[idx]\n",
    "        return input, label, weights, diss, transcriptt\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targetset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "101d8520",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use_own=False\n",
    "# sseq=sseqs[2].values[ss]\n",
    "# test_o=sseqs[0].values[ss]\n",
    "# test_o=\"ENST00000343053.6\"\n",
    "# sseq=\"ATTAGAGTCTGTGCTTCACTTCCGTTCCAGCCTCAGCGGCAGCTGGATCGCTCGACGGAGTGCCTCTGGTAGTTGGCCAAGACGCCGAATATCAAAATCTTCAGCGGCAGCTCCCACCAGGACTTATCCCAGAAAATTGCTGACCGCCTGGGCCTGGAGCTAGGCAAGGTGGTGACTAAGAAATTCAGCAACCAGGAGACCTGCGTGGAAATTGATGAGAGTGTGCGTGGAGAGGATGTCTACATCGTTCAGAGTGGTTGTGGCGAAATCAACGACAGTCTAATGGAGCTTTTGATCATGATTAATGCCTGCAAGATTGCTTCAGCTAGCCGAGTTACTGCAGTCATCCCATGCTTCCCTTATGCCCGACAGGATAAGAAGGATAAGAGCCGGTCCCCAATCTCTGCCAAGCTTGTTGCAAATATG\"\n",
    "# if use_own==False:\n",
    "#     sseq=fa[test_o]\n",
    "# my_padded_seq=\"P\"*padding_size+sseq+\"P\"*padding_size\n",
    "\n",
    "# pattern=re.compile(r'(?=(ATG|TTG|CTG|GTG|AAG|ACG|AGG|ATA|ATT|ATC))')\n",
    "# it=re.finditer(pattern,my_padded_seq)\n",
    "# pos3=[i.span()[0] for i in it][0:(TIS_max_num+3)]\n",
    "# pos2=pos3[0:(TIS_max_num+1)]\n",
    "# pos=pos2[0:TIS_max_num]\n",
    "# if len(pos2)==len(pos):\n",
    "#     pos2=pos2+[pos2[len(pos2)-1]+20]\n",
    "# my_seq=torch.stack([torch.tensor([dict[j] for j in list(my_padded_seq[(p-padding_size):(p+padding_size+1)])], dtype=torch.long) for p in pos])\n",
    "# my_target=torch.tensor([1 for j in range(len(pos))])\n",
    "# my_weight=torch.tensor([1]*len(pos))\n",
    "# pos1=[padding_size]+pos\n",
    "# my_dis=torch.stack([torch.tensor([pos1[i+1]-pos1[i] for i in range(len(pos1)-1)]),torch.tensor([pos2[i+1]-pos2[i] for i in range(len(pos2)-1)])],1)\n",
    "# my_dis=my_dis/sqrt_scale\n",
    "\n",
    "# print(test_o)\n",
    "# MyvalData = testoneMyDataset(inputset=[my_seq], targetset=[my_target], weightset=[my_weight], disset=[my_dis], transcriptset=[test_o])\n",
    "# Myvalloader = torch.utils.data.DataLoader(MyvalData, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn_padd, drop_last=True)\n",
    "# outputss=pd.DataFrame()\n",
    "# for t_run in range(1,11):\n",
    "#     writer_title='run_all_'+str(t_run)\n",
    "#     state = torch.load(save_path + \"\\\\\"+run_header+\"\\\\\"+writer_title+\".pkl\")\n",
    "#     Mymodel = MyLSTM(dropout=dropout, hidden_dim=hidden_dim, num_layers=num_layers, proj_size=proj_size)\n",
    "#     Mymodel.to(device)\n",
    "#     optimizer = optim.SGD(Mymodel.parameters(), lr=lr, momentum=momentum)\n",
    "#     Mymodel.load_state_dict(state['state_dict'])\n",
    "#     optimizer.load_state_dict(state['optimizer'])\n",
    "#     epoch = state['epoch']\n",
    "#     jj = state['jj']\n",
    "#     converge_level = state['converge_level']\n",
    "#     running_loss = state['running_loss']\n",
    "#     inspect_loss = state['inspect_loss']\n",
    "#     loss_ratio = state['loss_ratio']\n",
    "#     sigma_index = state['sigma_index']\n",
    "#     print(save_path + \"\\\\\"+run_header+\"\\\\\"+writer_title+\".pkl loaded.\")\n",
    "#     with torch.no_grad():\n",
    "#         iter0=iter(enumerate(Myvalloader, 0))\n",
    "#         iii, batch=next(iter0)\n",
    "#         inputs, sizes, labels, weights, diss, transcriptt = batch\n",
    "#         outputs = Mymodel(x=inputs, sizes=sizes, diss=diss)\n",
    "#         loss = criterion(outputs, labels, weights, torch.sum(sizes), loss_ratio)\n",
    "#         outputss['run_'+str(t_run)]=np.array(outputs.cpu().flatten(0))\n",
    "\n",
    "# outputss.to_csv('final_true_vs_predicts\\\\inspect_output.txt',sep='\\t',index=False)\n",
    "# np.savetxt('final_true_vs_predicts\\\\inspect.txt',np.array([1]*len(sseq)),header=test_o)\n",
    "# #pos=[i[1] for i in find_TIS(test_o)]\n",
    "\n",
    "# pattern=re.compile(r'(?=(ATG|TTG|CTG|GTG|AAG|ACG|AGG|ATA|ATT|ATC))')\n",
    "# it=re.finditer(pattern,sseq)\n",
    "# TIS=[(i.group(1),i.span()[0]) for i in it]\n",
    "# pos=[i[1] for i in TIS[0:TIS_max_num]]\n",
    "\n",
    "# np.savetxt('final_true_vs_predicts\\\\inspect_pos.txt',np.array(pos),header=test_o)\n",
    "# ss+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b40e7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323960\n"
     ]
    }
   ],
   "source": [
    "##突变模拟\n",
    "lnc_fa=read_fasta('all_lnc_new_plane.fa')\n",
    "print(len(lnc_fa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adb775c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyLSTM(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(5, 3), stride=(1, 1), padding=(2, 1))\n",
       "  (conv2): Conv2d(16, 16, kernel_size=(5, 3), stride=(1, 1), padding=(2, 1))\n",
       "  (fc0): Linear(in_features=4080, out_features=62, bias=True)\n",
       "  (rnn): LSTM(64, 128, num_layers=2, batch_first=True)\n",
       "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (lrelu1): LeakyReLU(negative_slope=0.05)\n",
       "  (lrelu2): LeakyReLU(negative_slope=0.05)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_run=1\n",
    "writer_title='run_all_'+str(t_run)\n",
    "state = torch.load(save_path + \"\\\\\"+run_header+\"\\\\\"+writer_title+\".pkl\")\n",
    "Mymodel1 = MyLSTM(dropout=dropout, hidden_dim=hidden_dim, num_layers=num_layers, proj_size=proj_size)\n",
    "Mymodel1.to(device)\n",
    "Mymodel1.load_state_dict(state['state_dict'])\n",
    "Mymodel1.eval()\n",
    "\n",
    "t_run=2\n",
    "writer_title='run_all_'+str(t_run)\n",
    "state = torch.load(save_path + \"\\\\\"+run_header+\"\\\\\"+writer_title+\".pkl\")\n",
    "Mymodel2 = MyLSTM(dropout=dropout, hidden_dim=hidden_dim, num_layers=num_layers, proj_size=proj_size)\n",
    "Mymodel2.to(device)\n",
    "Mymodel2.load_state_dict(state['state_dict'])\n",
    "Mymodel2.eval()\n",
    "\n",
    "t_run=3\n",
    "writer_title='run_all_'+str(t_run)\n",
    "state = torch.load(save_path + \"\\\\\"+run_header+\"\\\\\"+writer_title+\".pkl\")\n",
    "Mymodel3 = MyLSTM(dropout=dropout, hidden_dim=hidden_dim, num_layers=num_layers, proj_size=proj_size)\n",
    "Mymodel3.to(device)\n",
    "Mymodel3.load_state_dict(state['state_dict'])\n",
    "Mymodel3.eval()\n",
    "\n",
    "t_run=4\n",
    "writer_title='run_all_'+str(t_run)\n",
    "state = torch.load(save_path + \"\\\\\"+run_header+\"\\\\\"+writer_title+\".pkl\")\n",
    "Mymodel4 = MyLSTM(dropout=dropout, hidden_dim=hidden_dim, num_layers=num_layers, proj_size=proj_size)\n",
    "Mymodel4.to(device)\n",
    "Mymodel4.load_state_dict(state['state_dict'])\n",
    "Mymodel4.eval()\n",
    "\n",
    "t_run=5\n",
    "writer_title='run_all_'+str(t_run)\n",
    "state = torch.load(save_path + \"\\\\\"+run_header+\"\\\\\"+writer_title+\".pkl\")\n",
    "Mymodel5 = MyLSTM(dropout=dropout, hidden_dim=hidden_dim, num_layers=num_layers, proj_size=proj_size)\n",
    "Mymodel5.to(device)\n",
    "Mymodel5.load_state_dict(state['state_dict'])\n",
    "Mymodel5.eval()\n",
    "\n",
    "t_run=6\n",
    "writer_title='run_all_'+str(t_run)\n",
    "state = torch.load(save_path + \"\\\\\"+run_header+\"\\\\\"+writer_title+\".pkl\")\n",
    "Mymodel6 = MyLSTM(dropout=dropout, hidden_dim=hidden_dim, num_layers=num_layers, proj_size=proj_size)\n",
    "Mymodel6.to(device)\n",
    "Mymodel6.load_state_dict(state['state_dict'])\n",
    "Mymodel6.eval()\n",
    "\n",
    "t_run=7\n",
    "writer_title='run_all_'+str(t_run)\n",
    "state = torch.load(save_path + \"\\\\\"+run_header+\"\\\\\"+writer_title+\".pkl\")\n",
    "Mymodel7 = MyLSTM(dropout=dropout, hidden_dim=hidden_dim, num_layers=num_layers, proj_size=proj_size)\n",
    "Mymodel7.to(device)\n",
    "Mymodel7.load_state_dict(state['state_dict'])\n",
    "Mymodel7.eval()\n",
    "\n",
    "t_run=8\n",
    "writer_title='run_all_'+str(t_run)\n",
    "state = torch.load(save_path + \"\\\\\"+run_header+\"\\\\\"+writer_title+\".pkl\")\n",
    "Mymodel8 = MyLSTM(dropout=dropout, hidden_dim=hidden_dim, num_layers=num_layers, proj_size=proj_size)\n",
    "Mymodel8.to(device)\n",
    "Mymodel8.load_state_dict(state['state_dict'])\n",
    "Mymodel8.eval()\n",
    "\n",
    "t_run=9\n",
    "writer_title='run_all_'+str(t_run)\n",
    "state = torch.load(save_path + \"\\\\\"+run_header+\"\\\\\"+writer_title+\".pkl\")\n",
    "Mymodel9 = MyLSTM(dropout=dropout, hidden_dim=hidden_dim, num_layers=num_layers, proj_size=proj_size)\n",
    "Mymodel9.to(device)\n",
    "Mymodel9.load_state_dict(state['state_dict'])\n",
    "Mymodel9.eval()\n",
    "\n",
    "t_run=10\n",
    "writer_title='run_all_'+str(t_run)\n",
    "state = torch.load(save_path + \"\\\\\"+run_header+\"\\\\\"+writer_title+\".pkl\")\n",
    "Mymodel10 = MyLSTM(dropout=dropout, hidden_dim=hidden_dim, num_layers=num_layers, proj_size=proj_size)\n",
    "Mymodel10.to(device)\n",
    "Mymodel10.load_state_dict(state['state_dict'])\n",
    "Mymodel10.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae204ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HSALNT0183798',\n",
       " 'HSALNT0148586',\n",
       " 'HSALNT0209455',\n",
       " 'HSALNT0017544',\n",
       " 'HSALNT0100282',\n",
       " 'HSALNT0155210',\n",
       " 'HSALNT0388907',\n",
       " 'HSALNT0043156',\n",
       " 'HSALNT0253846',\n",
       " 'HSALNT0047803']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_lnc=list(pd.read_csv('expression_profiles_HPA/max10955lnc.txt',header=None)[0].values)\n",
    "my_lnc=list(set(lnc_fa.keys()) & set(my_lnc))\n",
    "my_lnc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5acf47f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10953\n"
     ]
    }
   ],
   "source": [
    "print(len(my_lnc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3696fdc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time used: 52374.52177000046 seconds\n",
      "average time: 4.7817512800146496 seconds\n"
     ]
    }
   ],
   "source": [
    "random.seed(121)\n",
    "dic1={\"A\": 0,\"T\": 1,\"G\": 2,\"C\": 3}\n",
    "ATGC=list('ATGC')\n",
    "HIS={}\n",
    "LNC=[]\n",
    "smut1=[]\n",
    "smut2=[]\n",
    "def mutate(x,a,b):\n",
    "    return x[:a]+ATGC[(dic1[x[a]]+b+1) % 4]+x[(a+1):]\n",
    "\n",
    "t0 = time.time()\n",
    "with torch.no_grad():\n",
    "    for lnc in my_lnc:\n",
    "        lnc_len=len(lnc_fa[lnc])\n",
    "        mut1_num=min(250,lnc_len)\n",
    "        his=[(-1,-1)]\n",
    "        padded_lnc_list=[\"P\"*padding_size+lnc_fa[lnc]+\"P\"*padding_size]\n",
    "\n",
    "        for i in range(mut1_num):\n",
    "            for j in range(3):\n",
    "                his.append((i,j))\n",
    "                padded_lnc_list.append(\"P\"*padding_size+mutate(lnc_fa[lnc],i,j)+\"P\"*padding_size)\n",
    "        my_seq = []\n",
    "        my_target=[]\n",
    "        my_weight=[]\n",
    "        my_dis=[]\n",
    "        for iii in range(len(padded_lnc_list)):\n",
    "            ii=padded_lnc_list[iii]\n",
    "            pattern=re.compile(r'(?=(ATG|TTG|CTG|GTG|AAG|ACG|AGG|ATA|ATT|ATC))')\n",
    "            it=re.finditer(pattern,ii)\n",
    "            pos3=[i.span()[0] for i in it][0:(TIS_max_num+3)]\n",
    "            pos2=pos3[0:(TIS_max_num+1)]\n",
    "            pos=pos2[0:TIS_max_num]\n",
    "            if len(pos2)==len(pos):\n",
    "                pos2=pos2+[pos2[len(pos2)-1]+20]\n",
    "            my_seq.append(torch.stack([torch.tensor([dict[j] for j in list(ii[(p-padding_size):(p+padding_size+1)])], dtype=torch.long) for p in pos]))\n",
    "            my_target.append(torch.tensor([1 for j in range(len(pos))]))\n",
    "            my_weight.append(torch.tensor([1]*len(pos)))\n",
    "            pos1=[padding_size]+pos\n",
    "            my_dis.append(torch.stack([torch.tensor([pos1[i+1]-pos1[i] for i in range(len(pos1)-1)]),torch.tensor([pos2[i+1]-pos2[i] for i in range(len(pos2)-1)])],1))\n",
    "        my_dis=[i/sqrt_scale for i in my_dis]\n",
    "        MyvalData = testoneMyDataset(inputset=my_seq, targetset=my_target, weightset=my_weight, disset=my_dis, transcriptset=his)\n",
    "        Myvalloader = torch.utils.data.DataLoader(MyvalData, batch_size=len(MyvalData), shuffle=False, num_workers=0, collate_fn=collate_fn_padd, drop_last=True)\n",
    "        iter0=iter(enumerate(Myvalloader, 0))\n",
    "        iii, batch=next(iter0)\n",
    "        inputs, sizes, labels, weights, diss, transcriptt = batch\n",
    "        outputs=torch.stack((Mymodel1(x=inputs, sizes=sizes, diss=diss),\n",
    "                            Mymodel2(x=inputs, sizes=sizes, diss=diss),\n",
    "                            Mymodel3(x=inputs, sizes=sizes, diss=diss),\n",
    "                            Mymodel4(x=inputs, sizes=sizes, diss=diss),\n",
    "                            Mymodel5(x=inputs, sizes=sizes, diss=diss),\n",
    "                            Mymodel6(x=inputs, sizes=sizes, diss=diss),\n",
    "                            Mymodel7(x=inputs, sizes=sizes, diss=diss),\n",
    "                            Mymodel8(x=inputs, sizes=sizes, diss=diss),\n",
    "                            Mymodel9(x=inputs, sizes=sizes, diss=diss),\n",
    "                            Mymodel10(x=inputs, sizes=sizes, diss=diss)))\n",
    "\n",
    "        outputs=torch.mean(outputs/torch.sum(torch.abs(outputs),2,keepdim=True),0,keepdim=False)\n",
    "        max_peak,indices = torch.max(outputs,1,keepdim=False)\n",
    "        outputs0=outputs[0][indices]\n",
    "        outputs0[outputs0<0.01]=0.01\n",
    "        outputs1=max_peak/outputs0\n",
    "        outputs2=max_peak/max(outputs[0])\n",
    "        \n",
    "        max_peak=np.array(max_peak.cpu())\n",
    "        outputs11=np.array(outputs1.cpu())\n",
    "        outputs22=np.array(outputs2.cpu())\n",
    "        HIS[lnc]=[(his[i][0],his[i][1],max_peak[i],outputs11[i],outputs22[i]) for i in range(len(his))]\n",
    "        outputs1[outputs1<1]=1\n",
    "        outputs2[outputs2<1]=1\n",
    "        LNC.append(lnc)\n",
    "        smut1.append(np.array(torch.mean(outputs1[1:]).cpu()))\n",
    "        smut2.append(np.array(torch.mean(outputs2[1:]).cpu()))\n",
    "t1 = time.time()\n",
    "print(\"time used:\",t1-t0,\"seconds\")\n",
    "print(\"average time:\",(t1-t0)/len(LNC),\"seconds\")\n",
    "pd.DataFrame({\"lncRNA\": LNC,\"self_foldchange\": smut1,\"overall_foldchange\": smut2}).to_csv(\"lnc_res.txt\",sep=\"\\t\")\n",
    "f=open('lnc_mutation_result.txt','wb')\n",
    "pickle.dump(HIS,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0103306c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-1, -1, 0.30230683, 1.0, 1.0),\n",
       " (0, 0, 0.31989497, 1.0581797, 1.0581797),\n",
       " (0, 1, 0.32658347, 21.068188, 1.0803046),\n",
       " (0, 2, 0.32697716, 1.081607, 1.081607),\n",
       " (1, 0, 0.30928198, 1.0230731, 1.0230731),\n",
       " (1, 1, 0.31292292, 20.186934, 1.0351169),\n",
       " (1, 2, 0.32571325, 21.012049, 1.0774261),\n",
       " (2, 0, 0.30073124, 0.9947881, 0.9947881),\n",
       " (2, 1, 0.28486934, 0.94231856, 0.94231856),\n",
       " (2, 2, 0.30588508, 19.732916, 1.0118365),\n",
       " (3, 0, 0.26456642, 0.8751586, 0.8751586),\n",
       " (3, 1, 0.29339495, 0.97052044, 0.97052044),\n",
       " (3, 2, 0.3261601, 1.0789042, 1.0789042),\n",
       " (4, 0, 0.30475062, 1.0080838, 1.0080838),\n",
       " (4, 1, 0.26919207, 0.8904598, 0.8904598),\n",
       " (4, 2, 0.27387664, 0.90595585, 0.90595585),\n",
       " (5, 0, 0.3223824, 1.0664079, 1.0664079),\n",
       " (5, 1, 0.28005254, 2.7420347, 0.9263851),\n",
       " (5, 2, 0.2653809, 2.5983822, 0.8778528),\n",
       " (6, 0, 0.2879587, 0.9525379, 0.9525379),\n",
       " (6, 1, 0.23265074, 2.2779167, 0.7695848),\n",
       " (6, 2, 0.25319597, 0.8375463, 0.8375463),\n",
       " (7, 0, 0.25420165, 25.420166, 0.840873),\n",
       " (7, 1, 0.32389924, 1.0714254, 1.0714254),\n",
       " (7, 2, 0.28954706, 18.678936, 0.957792),\n",
       " (8, 0, 0.3028159, 19.534922, 1.001684),\n",
       " (8, 1, 0.27332267, 0.90412337, 0.90412337),\n",
       " (8, 2, 0.27833495, 0.9207035, 0.9207035),\n",
       " (9, 0, 0.23365001, 15.0729685, 0.77289027),\n",
       " (9, 1, 0.2783313, 0.92069143, 0.92069143),\n",
       " (9, 2, 0.3203856, 20.668358, 1.0598028),\n",
       " (10, 0, 0.3075138, 1.0172241, 1.0172241),\n",
       " (10, 1, 0.35640606, 1.1789547, 1.1789547),\n",
       " (10, 2, 0.2964428, 19.123787, 0.9806024),\n",
       " (11, 0, 0.27236423, 17.570457, 0.90095294),\n",
       " (11, 1, 0.3372096, 1.1154548, 1.1154548),\n",
       " (11, 2, 0.2805716, 18.099922, 0.92810214),\n",
       " (12, 0, 0.32976267, 21.273281, 1.0908211),\n",
       " (12, 1, 0.35396862, 1.1708919, 1.1708919),\n",
       " (12, 2, 0.3276217, 1.083739, 1.083739),\n",
       " (13, 0, 0.27588025, 0.9125836, 0.9125836),\n",
       " (13, 1, 0.3003025, 0.9933699, 0.9933699),\n",
       " (13, 2, 0.26673734, 17.20746, 0.8823398),\n",
       " (14, 0, 0.18042368, 0.59682304, 0.59682304),\n",
       " (14, 1, 0.2706156, 0.89516866, 0.89516866),\n",
       " (14, 2, 0.32841548, 1.0863647, 1.0863647),\n",
       " (15, 0, 0.31797716, 3.11336, 1.0518359),\n",
       " (15, 1, 0.32441127, 3.1763573, 1.0731193),\n",
       " (15, 2, 0.3307344, 3.238268, 1.0940355),\n",
       " (16, 0, 0.42262322, 4.1379647, 1.3979943),\n",
       " (16, 1, 0.3565045, 3.4905868, 1.1792803),\n",
       " (16, 2, 0.43805367, 4.2890463, 1.4490366),\n",
       " (17, 0, 0.30902788, 1.0222325, 1.0222325),\n",
       " (17, 1, 0.31257486, 1.0339656, 1.0339656),\n",
       " (17, 2, 0.24313922, 0.8042796, 0.8042796),\n",
       " (18, 0, 0.33620486, 1.1121312, 1.1121312),\n",
       " (18, 1, 0.3417627, 1.1305159, 1.1305159),\n",
       " (18, 2, 0.32271048, 1.0674932, 1.0674932),\n",
       " (19, 0, 0.3909346, 3.8276966, 1.2931715),\n",
       " (19, 1, 0.36341658, 3.5582638, 1.2021447),\n",
       " (19, 2, 0.3602545, 3.5273035, 1.191685),\n",
       " (20, 0, 0.36350393, 3.5591192, 1.2024337),\n",
       " (20, 1, 0.35601687, 3.4858122, 1.1776673),\n",
       " (20, 2, 0.36300632, 1.2007877, 1.2007877),\n",
       " (21, 0, 0.63066095, 5.2387657, 2.0861619),\n",
       " (21, 1, 0.29908934, 0.9893569, 0.9893569),\n",
       " (21, 2, 0.2866021, 0.9480504, 0.9480504),\n",
       " (22, 0, 0.573965, 5.619774, 1.8986174),\n",
       " (22, 1, 0.32572615, 1.0774688, 1.0774688),\n",
       " (22, 2, 0.34212688, 22.070906, 1.1317207),\n",
       " (23, 0, 0.2977982, 2.9157848, 0.98508584),\n",
       " (23, 1, 0.2965111, 2.9031827, 0.98082834),\n",
       " (23, 2, 0.30388525, 2.975384, 1.0052212),\n",
       " (24, 0, 0.36141092, 3.5386262, 1.1955103),\n",
       " (24, 1, 0.38867557, 3.8055782, 1.2856989),\n",
       " (24, 2, 0.30297938, 2.9665146, 1.0022247),\n",
       " (25, 0, 0.43562028, 1.4409872, 1.4409872),\n",
       " (25, 1, 0.25853357, 0.85520256, 0.85520256),\n",
       " (25, 2, 0.33024174, 3.2334442, 1.0924058),\n",
       " (26, 0, 0.21790008, 14.056927, 0.7207911),\n",
       " (26, 1, 0.2443616, 0.8083231, 0.8083231),\n",
       " (26, 2, 0.20921712, 0.69206876, 0.69206876),\n",
       " (27, 0, 0.31724426, 1.0494115, 1.0494115),\n",
       " (27, 1, 0.3687612, 1.2198243, 1.2198243),\n",
       " (27, 2, 0.37202063, 1.2306061, 1.2306061),\n",
       " (28, 0, 0.39556757, 3.276101, 1.308497),\n",
       " (28, 1, 0.461541, 46.154102, 1.5267303),\n",
       " (28, 2, 0.3218964, 2.6659546, 1.0648003),\n",
       " (29, 0, 0.45902038, 3.801619, 1.5183923),\n",
       " (29, 1, 0.36326438, 3.0085652, 1.2016413),\n",
       " (29, 2, 0.4207511, 42.07511, 1.3918015),\n",
       " (30, 0, 0.32393512, 2.6828394, 1.0715442),\n",
       " (30, 1, 0.38702497, 38.7025, 1.2802389),\n",
       " (30, 2, 0.32552654, 32.552654, 1.0768085),\n",
       " (31, 0, 0.22277586, 0.7369197, 0.7369197),\n",
       " (31, 1, 0.18626021, 0.6161297, 0.6161297),\n",
       " (31, 2, 0.15590827, 1.5265203, 0.5157286),\n",
       " (32, 0, 0.38820246, 1.2841339, 1.2841339),\n",
       " (32, 1, 0.34885237, 1.1539679, 1.1539679),\n",
       " (32, 2, 0.21426634, 0.7087711, 0.7087711),\n",
       " (33, 0, 0.29858515, 0.9876891, 0.9876891),\n",
       " (33, 1, 0.29972863, 0.9914716, 0.9914716),\n",
       " (33, 2, 0.36468336, 1.2063352, 1.2063352),\n",
       " (34, 0, 0.37894052, 1.2534964, 1.2534964),\n",
       " (34, 1, 0.3056431, 1.011036, 1.011036),\n",
       " (34, 2, 0.31419206, 1.0393151, 1.0393151),\n",
       " (35, 0, 0.34172425, 1.1303887, 1.1303887),\n",
       " (35, 1, 0.30894473, 1.0219575, 1.0219575),\n",
       " (35, 2, 0.30021662, 0.9930858, 0.9930858),\n",
       " (36, 0, 0.32935694, 1.089479, 1.089479),\n",
       " (36, 1, 0.23386316, 0.77359533, 0.77359533),\n",
       " (36, 2, 0.28782687, 0.95210177, 0.95210177),\n",
       " (37, 0, 0.24151278, 0.7988995, 0.7988995),\n",
       " (37, 1, 0.29772735, 0.98485154, 0.98485154),\n",
       " (37, 2, 0.28662798, 0.948136, 0.948136),\n",
       " (38, 0, 0.31593975, 1.0450963, 1.0450963),\n",
       " (38, 1, 0.25994587, 0.8598743, 0.8598743),\n",
       " (38, 2, 0.27149528, 0.89807856, 0.89807856),\n",
       " (39, 0, 0.28816575, 0.95322275, 0.95322275),\n",
       " (39, 1, 0.31959805, 1.0571976, 1.0571976),\n",
       " (39, 2, 0.34085703, 1.1275201, 1.1275201),\n",
       " (40, 0, 0.29992226, 0.9921121, 0.9921121),\n",
       " (40, 1, 0.29493743, 0.9756228, 0.9756228),\n",
       " (40, 2, 0.24733774, 0.8181679, 0.8181679),\n",
       " (41, 0, 0.32735974, 1.0828724, 1.0828724),\n",
       " (41, 1, 0.30965286, 1.0243, 1.0243),\n",
       " (41, 2, 0.30954453, 1.0239416, 1.0239416),\n",
       " (42, 0, 0.2841942, 0.9400853, 0.9400853),\n",
       " (42, 1, 0.25899243, 0.8567204, 0.8567204),\n",
       " (42, 2, 0.2812572, 0.93037003, 0.93037003),\n",
       " (43, 0, 0.28467235, 0.94166696, 0.94166696),\n",
       " (43, 1, 0.255749, 0.84599143, 0.84599143),\n",
       " (43, 2, 0.27587196, 0.9125562, 0.9125562),\n",
       " (44, 0, 0.23151676, 0.76583374, 0.76583374),\n",
       " (44, 1, 0.25630566, 0.84783286, 0.84783286),\n",
       " (44, 2, 0.28135046, 0.9306785, 0.9306785),\n",
       " (45, 0, 0.29229015, 0.96686584, 0.96686584),\n",
       " (45, 1, 0.3212007, 1.0624989, 1.0624989),\n",
       " (45, 2, 0.2574235, 0.8515305, 0.8515305),\n",
       " (46, 0, 0.2363935, 0.78196543, 0.78196543),\n",
       " (46, 1, 0.27866858, 0.9218071, 0.9218071),\n",
       " (46, 2, 0.26344484, 0.8714485, 0.8714485),\n",
       " (47, 0, 0.32955202, 1.0901242, 1.0901242),\n",
       " (47, 1, 0.29557553, 0.97773355, 0.97773355),\n",
       " (47, 2, 0.31897235, 1.0551279, 1.0551279),\n",
       " (48, 0, 0.27721396, 2.2958934, 0.91699535),\n",
       " (48, 1, 0.2915778, 0.9645094, 0.9645094),\n",
       " (48, 2, 0.27743497, 0.91772646, 0.91772646),\n",
       " (49, 0, 0.32818243, 1.0855938, 1.0855938),\n",
       " (49, 1, 0.31388435, 1.0382973, 1.0382973),\n",
       " (49, 2, 0.306415, 1.0135894, 1.0135894),\n",
       " (50, 0, 0.3344063, 1.1061817, 1.1061817),\n",
       " (50, 1, 0.33744186, 1.1162231, 1.1162231),\n",
       " (50, 2, 0.36564133, 1.209504, 1.209504),\n",
       " (51, 0, 0.33325896, 1.1023865, 1.1023865),\n",
       " (51, 1, 0.3184534, 1.0534112, 1.0534112),\n",
       " (51, 2, 0.33335882, 1.1027168, 1.1027168),\n",
       " (52, 0, 0.32237145, 1.0663717, 1.0663717),\n",
       " (52, 1, 0.30489442, 1.0085595, 1.0085595),\n",
       " (52, 2, 0.3071117, 1.015894, 1.015894),\n",
       " (53, 0, 0.27967536, 0.9251374, 0.9251374),\n",
       " (53, 1, 0.33561203, 1.1101701, 1.1101701),\n",
       " (53, 2, 0.31764525, 1.050738, 1.050738),\n",
       " (54, 0, 0.31673476, 1.0477262, 1.0477262),\n",
       " (54, 1, 0.31182605, 1.0314887, 1.0314887),\n",
       " (54, 2, 0.31903407, 1.055332, 1.055332),\n",
       " (55, 0, 0.28114697, 0.9300054, 0.9300054),\n",
       " (55, 1, 0.27317396, 0.90363145, 0.90363145),\n",
       " (55, 2, 0.27538615, 0.9109492, 0.9109492),\n",
       " (56, 0, 0.29830512, 0.98676276, 0.98676276),\n",
       " (56, 1, 0.29796046, 0.98562264, 0.98562264),\n",
       " (56, 2, 0.29811272, 0.9861263, 0.9861263),\n",
       " (57, 0, 0.2959327, 0.97891504, 0.97891504),\n",
       " (57, 1, 0.29726145, 0.9833104, 0.9833104),\n",
       " (57, 2, 0.30434147, 1.0067303, 1.0067303),\n",
       " (58, 0, 0.3052675, 1.0097936, 1.0097936),\n",
       " (58, 1, 0.28785682, 0.95220083, 0.95220083),\n",
       " (58, 2, 0.3085556, 1.0206703, 1.0206703),\n",
       " (59, 0, 0.29616123, 0.979671, 0.979671),\n",
       " (59, 1, 0.29690626, 0.9821355, 0.9821355),\n",
       " (59, 2, 0.29669923, 0.9814506, 0.9814506),\n",
       " (60, 0, 0.30017996, 0.9929645, 0.9929645),\n",
       " (60, 1, 0.31174657, 1.0312257, 1.0312257),\n",
       " (60, 2, 0.31075996, 1.0279621, 1.0279621),\n",
       " (61, 0, 0.3056515, 1.0110638, 1.0110638),\n",
       " (61, 1, 0.307135, 1.0159711, 1.0159711),\n",
       " (61, 2, 0.30514884, 1.0094011, 1.0094011),\n",
       " (62, 0, 0.30302224, 1.0023665, 1.0023665),\n",
       " (62, 1, 0.31059557, 1.0274183, 1.0274183),\n",
       " (62, 2, 0.29665613, 0.98130804, 0.98130804),\n",
       " (63, 0, 0.30833477, 1.0199398, 1.0199398),\n",
       " (63, 1, 0.30415067, 1.0060992, 1.0060992),\n",
       " (63, 2, 0.30784926, 1.0183338, 1.0183338),\n",
       " (64, 0, 0.31047085, 1.0270058, 1.0270058),\n",
       " (64, 1, 0.31001148, 1.0254862, 1.0254862),\n",
       " (64, 2, 0.30571958, 1.011289, 1.011289),\n",
       " (65, 0, 0.29086205, 0.9621419, 0.9621419),\n",
       " (65, 1, 0.29613042, 0.9795691, 0.9795691),\n",
       " (65, 2, 0.2942822, 0.9734553, 0.9734553),\n",
       " (66, 0, 0.31041455, 1.0268195, 1.0268195),\n",
       " (66, 1, 0.30605513, 1.012399, 1.012399),\n",
       " (66, 2, 0.3089982, 1.0221343, 1.0221343),\n",
       " (67, 0, 0.29483458, 0.97528255, 0.97528255),\n",
       " (67, 1, 0.2952281, 0.97658426, 0.97658426),\n",
       " (67, 2, 0.30420732, 1.0062866, 1.0062866),\n",
       " (68, 0, 0.29307568, 0.9694643, 0.9694643),\n",
       " (68, 1, 0.29448035, 0.97411084, 0.97411084),\n",
       " (68, 2, 0.29411432, 0.97290003, 0.97290003),\n",
       " (69, 0, 0.3135902, 1.0373242, 1.0373242),\n",
       " (69, 1, 0.31898937, 1.0551841, 1.0551841),\n",
       " (69, 2, 0.30806932, 1.0190617, 1.0190617),\n",
       " (70, 0, 0.30302674, 1.0023813, 1.0023813),\n",
       " (70, 1, 0.3101898, 1.0260761, 1.0260761),\n",
       " (70, 2, 0.31231746, 1.0331142, 1.0331142),\n",
       " (71, 0, 0.31090394, 1.0284383, 1.0284383),\n",
       " (71, 1, 0.31098452, 1.0287049, 1.0287049),\n",
       " (71, 2, 0.30557427, 1.0108083, 1.0108083),\n",
       " (72, 0, 0.3094966, 1.0237831, 1.0237831),\n",
       " (72, 1, 0.30644867, 1.0137007, 1.0137007),\n",
       " (72, 2, 0.31036982, 1.0266715, 1.0266715),\n",
       " (73, 0, 0.29980907, 0.99173766, 0.99173766),\n",
       " (73, 1, 0.30083558, 0.9951333, 0.9951333),\n",
       " (73, 2, 0.308219, 1.0195568, 1.0195568),\n",
       " (74, 0, 0.30343315, 1.0037258, 1.0037258),\n",
       " (74, 1, 0.30015698, 0.9928885, 0.9928885),\n",
       " (74, 2, 0.30142528, 0.9970839, 0.9970839),\n",
       " (75, 0, 0.3032832, 1.0032297, 1.0032297),\n",
       " (75, 1, 0.30287215, 1.00187, 1.00187),\n",
       " (75, 2, 0.3049901, 1.008876, 1.008876),\n",
       " (76, 0, 0.29936942, 0.99028337, 0.99028337),\n",
       " (76, 1, 0.2969477, 0.9822725, 0.9822725),\n",
       " (76, 2, 0.29464793, 0.97466516, 0.97466516),\n",
       " (77, 0, 0.29145274, 0.9640958, 0.9640958),\n",
       " (77, 1, 0.2956397, 0.9779458, 0.9779458),\n",
       " (77, 2, 0.29245, 0.96739465, 0.96739465),\n",
       " (78, 0, 0.29879698, 0.9883898, 0.9883898),\n",
       " (78, 1, 0.30117792, 0.99626565, 0.99626565),\n",
       " (78, 2, 0.30022442, 0.9931116, 0.9931116),\n",
       " (79, 0, 0.2977853, 0.9850433, 0.9850433),\n",
       " (79, 1, 0.30179745, 0.99831504, 0.99831504),\n",
       " (79, 2, 0.30024943, 0.99319434, 0.99319434),\n",
       " (80, 0, 0.29984945, 0.99187124, 0.99187124),\n",
       " (80, 1, 0.29953134, 0.990819, 0.990819),\n",
       " (80, 2, 0.2981639, 0.9862956, 0.9862956),\n",
       " (81, 0, 0.3029333, 1.0020723, 1.0020723),\n",
       " (81, 1, 0.30315912, 1.0028193, 1.0028193),\n",
       " (81, 2, 0.30223122, 0.9997499, 0.9997499),\n",
       " (82, 0, 0.3036061, 1.0042979, 1.0042979),\n",
       " (82, 1, 0.30244124, 1.0004447, 1.0004447),\n",
       " (82, 2, 0.30238986, 1.0002747, 1.0002747),\n",
       " (83, 0, 0.3070489, 1.0156863, 1.0156863),\n",
       " (83, 1, 0.3043332, 1.006703, 1.006703),\n",
       " (83, 2, 0.30434188, 1.0067317, 1.0067317),\n",
       " (84, 0, 0.30676016, 1.0147312, 1.0147312),\n",
       " (84, 1, 0.30480653, 1.0082687, 1.0082687),\n",
       " (84, 2, 0.3040272, 1.0056908, 1.0056908),\n",
       " (85, 0, 0.30327415, 1.0031998, 1.0031998),\n",
       " (85, 1, 0.30350274, 1.003956, 1.003956),\n",
       " (85, 2, 0.3036855, 1.0045605, 1.0045605),\n",
       " (86, 0, 0.30699214, 1.0154985, 1.0154985),\n",
       " (86, 1, 0.30618855, 1.0128404, 1.0128404),\n",
       " (86, 2, 0.30545288, 1.0104069, 1.0104069),\n",
       " (87, 0, 0.30556342, 1.0107725, 1.0107725),\n",
       " (87, 1, 0.30197975, 0.99891806, 0.99891806),\n",
       " (87, 2, 0.3056152, 1.0109437, 1.0109437),\n",
       " (88, 0, 0.29851785, 0.98746645, 0.98746645),\n",
       " (88, 1, 0.3001024, 0.992708, 0.992708),\n",
       " (88, 2, 0.2968159, 0.98183656, 0.98183656),\n",
       " (89, 0, 0.3025686, 1.0008659, 1.0008659),\n",
       " (89, 1, 0.30448455, 1.0072037, 1.0072037),\n",
       " (89, 2, 0.30003944, 0.9924997, 0.9924997),\n",
       " (90, 0, 0.30316326, 1.002833, 1.002833),\n",
       " (90, 1, 0.30269986, 1.0013001, 1.0013001),\n",
       " (90, 2, 0.3039411, 1.005406, 1.005406),\n",
       " (91, 0, 0.30367392, 1.0045222, 1.0045222),\n",
       " (91, 1, 0.2988264, 0.98848706, 0.98848706),\n",
       " (91, 2, 0.3004478, 0.99385047, 0.99385047),\n",
       " (92, 0, 0.3026782, 1.0012285, 1.0012285),\n",
       " (92, 1, 0.30160522, 0.9976792, 0.9976792),\n",
       " (92, 2, 0.3011005, 0.9960095, 0.9960095),\n",
       " (93, 0, 0.30318192, 1.0028946, 1.0028946),\n",
       " (93, 1, 0.30350724, 1.0039709, 1.0039709),\n",
       " (93, 2, 0.30384716, 1.0050952, 1.0050952),\n",
       " (94, 0, 0.31013495, 1.0258946, 1.0258946),\n",
       " (94, 1, 0.30331168, 1.0033239, 1.0033239),\n",
       " (94, 2, 0.30868918, 1.0211122, 1.0211122),\n",
       " (95, 0, 0.3089954, 1.0221251, 1.0221251),\n",
       " (95, 1, 0.29911694, 0.9894482, 0.9894482),\n",
       " (95, 2, 0.30311915, 1.0026871, 1.0026871),\n",
       " (96, 0, 0.30847415, 1.0204009, 1.0204009),\n",
       " (96, 1, 0.30841443, 1.0202034, 1.0202034),\n",
       " (96, 2, 0.30743068, 1.0169492, 1.0169492),\n",
       " (97, 0, 0.29935446, 0.9902339, 0.9902339),\n",
       " (97, 1, 0.30092117, 0.9954164, 0.9954164),\n",
       " (97, 2, 0.294991, 0.9757999, 0.9757999),\n",
       " (98, 0, 0.3048461, 1.0083997, 1.0083997),\n",
       " (98, 1, 0.30164656, 0.9978159, 0.9978159),\n",
       " (98, 2, 0.30180725, 0.99834746, 0.99834746),\n",
       " (99, 0, 0.3080468, 1.0189872, 1.0189872),\n",
       " (99, 1, 0.30491525, 1.0086284, 1.0086284),\n",
       " (99, 2, 0.3094667, 1.023684, 1.023684),\n",
       " (100, 0, 0.3024105, 1.000343, 1.000343),\n",
       " (100, 1, 0.30423173, 1.0063673, 1.0063673),\n",
       " (100, 2, 0.30087197, 0.9952536, 0.9952536),\n",
       " (101, 0, 0.30395922, 1.005466, 1.005466),\n",
       " (101, 1, 0.30725202, 1.0163581, 1.0163581),\n",
       " (101, 2, 0.3043726, 1.0068334, 1.0068334),\n",
       " (102, 0, 0.31105265, 1.0289303, 1.0289303),\n",
       " (102, 1, 0.30651203, 1.0139104, 1.0139104),\n",
       " (102, 2, 0.31000948, 1.0254796, 1.0254796),\n",
       " (103, 0, 0.30805078, 1.0190004, 1.0190004),\n",
       " (103, 1, 0.30442446, 1.0070049, 1.0070049),\n",
       " (103, 2, 0.3114924, 1.030385, 1.030385),\n",
       " (104, 0, 0.30167043, 0.9978949, 0.9978949),\n",
       " (104, 1, 0.30581713, 1.0116117, 1.0116117),\n",
       " (104, 2, 0.30190632, 0.99867517, 0.99867517),\n",
       " (105, 0, 0.29866987, 0.98796934, 0.98796934),\n",
       " (105, 1, 0.30199224, 0.99895936, 0.99895936),\n",
       " (105, 2, 0.30134788, 0.9968279, 0.9968279),\n",
       " (106, 0, 0.30549464, 1.0105449, 1.0105449),\n",
       " (106, 1, 0.30290708, 1.0019855, 1.0019855),\n",
       " (106, 2, 0.3026888, 1.0012635, 1.0012635),\n",
       " (107, 0, 0.29098114, 0.9625358, 0.9625358),\n",
       " (107, 1, 0.30104175, 0.9958153, 0.9958153),\n",
       " (107, 2, 0.30023256, 0.99313855, 0.99313855),\n",
       " (108, 0, 0.30404362, 1.0057452, 1.0057452),\n",
       " (108, 1, 0.30061135, 0.9943915, 0.9943915),\n",
       " (108, 2, 0.29753113, 0.98420244, 0.98420244),\n",
       " (109, 0, 0.3056266, 1.0109814, 1.0109814),\n",
       " (109, 1, 0.3056535, 1.0110705, 1.0110705),\n",
       " (109, 2, 0.30220875, 0.9996756, 0.9996756),\n",
       " (110, 0, 0.29784915, 0.98525447, 0.98525447),\n",
       " (110, 1, 0.30058816, 0.9943148, 0.9943148),\n",
       " (110, 2, 0.29595414, 0.97898597, 0.97898597),\n",
       " (111, 0, 0.29766166, 0.9846343, 0.9846343),\n",
       " (111, 1, 0.29852936, 0.9875045, 0.9875045),\n",
       " (111, 2, 0.30135533, 0.9968525, 0.9968525),\n",
       " (112, 0, 0.300854, 0.9951942, 0.9951942),\n",
       " (112, 1, 0.304327, 1.0066825, 1.0066825),\n",
       " (112, 2, 0.3017045, 0.99800754, 0.99800754),\n",
       " (113, 0, 0.30099806, 0.99567074, 0.99567074),\n",
       " (113, 1, 0.30443984, 1.0070558, 1.0070558),\n",
       " (113, 2, 0.29516402, 0.9763723, 0.9763723),\n",
       " (114, 0, 0.30264983, 1.0011346, 1.0011346),\n",
       " (114, 1, 0.3012476, 0.99649614, 0.99649614),\n",
       " (114, 2, 0.30395085, 1.0054382, 1.0054382),\n",
       " (115, 0, 0.30038938, 0.9936573, 0.9936573),\n",
       " (115, 1, 0.30014575, 0.9928514, 0.9928514),\n",
       " (115, 2, 0.2998107, 0.9917431, 0.9917431),\n",
       " (116, 0, 0.30379018, 1.0049068, 1.0049068),\n",
       " (116, 1, 0.2912125, 0.96330106, 0.96330106),\n",
       " (116, 2, 0.2958718, 0.9787136, 0.9787136),\n",
       " (117, 0, 0.29278502, 0.9685028, 0.9685028),\n",
       " (117, 1, 0.28766924, 0.95158035, 0.95158035),\n",
       " (117, 2, 0.28868422, 0.9549378, 0.9549378),\n",
       " (118, 0, 0.27917144, 0.9234705, 0.9234705),\n",
       " (118, 1, 0.29845443, 0.98725665, 0.98725665),\n",
       " (118, 2, 0.2808939, 0.92916816, 0.92916816),\n",
       " (119, 0, 0.29932877, 0.9901489, 0.9901489),\n",
       " (119, 1, 0.29335967, 0.9704037, 0.9704037),\n",
       " (119, 2, 0.30114123, 0.9961443, 0.9961443),\n",
       " (120, 0, 0.30756268, 1.0173858, 1.0173858),\n",
       " (120, 1, 0.3019144, 0.9987019, 0.9987019),\n",
       " (120, 2, 0.3057471, 1.0113801, 1.0113801),\n",
       " (121, 0, 0.3130664, 1.0355915, 1.0355915),\n",
       " (121, 1, 0.3084486, 1.0203164, 1.0203164),\n",
       " (121, 2, 0.31001934, 1.0255122, 1.0255122),\n",
       " (122, 0, 0.30631652, 1.0132637, 1.0132637),\n",
       " (122, 1, 0.30669495, 1.0145155, 1.0145155),\n",
       " (122, 2, 0.30551663, 1.0106177, 1.0106177),\n",
       " (123, 0, 0.30173776, 0.99811757, 0.99811757),\n",
       " (123, 1, 0.29976398, 0.9915885, 0.9915885),\n",
       " (123, 2, 0.30428168, 1.0065325, 1.0065325),\n",
       " (124, 0, 0.30428097, 1.0065303, 1.0065303),\n",
       " (124, 1, 0.30613732, 1.0126709, 1.0126709),\n",
       " (124, 2, 0.30765685, 1.0176973, 1.0176973),\n",
       " (125, 0, 0.3053766, 1.0101545, 1.0101545),\n",
       " (125, 1, 0.30616963, 1.0127777, 1.0127777),\n",
       " (125, 2, 0.30708358, 1.015801, 1.015801),\n",
       " (126, 0, 0.30516157, 1.0094432, 1.0094432),\n",
       " (126, 1, 0.30542248, 1.0103062, 1.0103062),\n",
       " (126, 2, 0.3086981, 1.0211416, 1.0211416),\n",
       " (127, 0, 0.30387464, 1.0051862, 1.0051862),\n",
       " (127, 1, 0.3036347, 1.0043925, 1.0043925),\n",
       " (127, 2, 0.3053101, 1.0099345, 1.0099345),\n",
       " (128, 0, 0.2986244, 0.9878189, 0.9878189),\n",
       " (128, 1, 0.2946503, 0.9746729, 0.9746729),\n",
       " (128, 2, 0.3037098, 1.0046409, 1.0046409),\n",
       " (129, 0, 0.3084251, 1.0202386, 1.0202386),\n",
       " (129, 1, 0.3065462, 1.0140234, 1.0140234),\n",
       " (129, 2, 0.30639097, 1.0135099, 1.0135099),\n",
       " (130, 0, 0.3106441, 1.0275788, 1.0275788),\n",
       " (130, 1, 0.30362785, 1.0043697, 1.0043697),\n",
       " (130, 2, 0.31040666, 1.0267934, 1.0267934),\n",
       " (131, 0, 0.3056736, 1.0111369, 1.0111369),\n",
       " (131, 1, 0.30019408, 0.99301124, 0.99301124),\n",
       " (131, 2, 0.30586633, 1.0117744, 1.0117744),\n",
       " (132, 0, 0.30800033, 1.0188335, 1.0188335),\n",
       " (132, 1, 0.30937773, 1.0233898, 1.0233898),\n",
       " (132, 2, 0.30964515, 1.0242743, 1.0242743),\n",
       " (133, 0, 0.31247878, 1.0336478, 1.0336478),\n",
       " (133, 1, 0.31298825, 1.035333, 1.035333),\n",
       " (133, 2, 0.31138498, 1.0300295, 1.0300295),\n",
       " (134, 0, 0.3128251, 1.0347934, 1.0347934),\n",
       " (134, 1, 0.313292, 1.0363379, 1.0363379),\n",
       " (134, 2, 0.31317723, 1.0359582, 1.0359582),\n",
       " (135, 0, 0.31339785, 1.036688, 1.036688),\n",
       " (135, 1, 0.31359002, 1.0373236, 1.0373236),\n",
       " (135, 2, 0.31355888, 1.0372206, 1.0372206),\n",
       " (136, 0, 0.3009492, 0.995509, 0.995509),\n",
       " (136, 1, 0.3000859, 0.99265337, 0.99265337),\n",
       " (136, 2, 0.3003303, 0.99346185, 0.99346185),\n",
       " (137, 0, 0.2988339, 0.9885119, 0.9885119),\n",
       " (137, 1, 0.2947949, 0.97515124, 0.97515124),\n",
       " (137, 2, 0.2968715, 0.9820205, 0.9820205),\n",
       " (138, 0, 0.30310798, 1.0026501, 1.0026501),\n",
       " (138, 1, 0.3031734, 1.0028665, 1.0028665),\n",
       " (138, 2, 0.3045671, 1.0074767, 1.0074767),\n",
       " (139, 0, 0.29946193, 0.9905894, 0.9905894),\n",
       " (139, 1, 0.30075, 0.9948501, 0.9948501),\n",
       " (139, 2, 0.29961988, 0.9911119, 0.9911119),\n",
       " (140, 0, 0.30224606, 0.999799, 0.999799),\n",
       " (140, 1, 0.30330795, 1.0033116, 1.0033116),\n",
       " (140, 2, 0.30302602, 1.0023791, 1.0023791),\n",
       " (141, 0, 0.3057613, 1.011427, 1.011427),\n",
       " (141, 1, 0.30304748, 1.00245, 1.00245),\n",
       " (141, 2, 0.30565667, 1.011081, 1.011081),\n",
       " (142, 0, 0.30400577, 1.0056199, 1.0056199),\n",
       " (142, 1, 0.30067095, 0.9945887, 0.9945887),\n",
       " (142, 2, 0.30309483, 1.0026066, 1.0026066),\n",
       " (143, 0, 0.30146247, 0.9972069, 0.9972069),\n",
       " (143, 1, 0.29760972, 0.98446244, 0.98446244),\n",
       " (143, 2, 0.30308115, 1.0025613, 1.0025613),\n",
       " (144, 0, 0.30303654, 1.0024137, 1.0024137),\n",
       " (144, 1, 0.3057664, 1.0114439, 1.0114439),\n",
       " (144, 2, 0.30528936, 1.0098659, 1.0098659),\n",
       " (145, 0, 0.30288097, 1.0018992, 1.0018992),\n",
       " (145, 1, 0.3014377, 0.997125, 0.997125),\n",
       " (145, 2, 0.30474168, 1.0080543, 1.0080543),\n",
       " (146, 0, 0.3040721, 1.0058393, 1.0058393),\n",
       " (146, 1, 0.30696243, 1.0154003, 1.0154003),\n",
       " (146, 2, 0.3049472, 1.0087341, 1.0087341),\n",
       " (147, 0, 0.3005528, 0.9941978, 0.9941978),\n",
       " (147, 1, 0.29943615, 0.9905041, 0.9905041),\n",
       " (147, 2, 0.29984257, 0.99184847, 0.99184847),\n",
       " (148, 0, 0.30723888, 1.0163147, 1.0163147),\n",
       " (148, 1, 0.30710772, 1.0158808, 1.0158808),\n",
       " (148, 2, 0.30315876, 1.0028181, 1.0028181),\n",
       " (149, 0, 0.30041766, 0.9937508, 0.9937508),\n",
       " (149, 1, 0.29605645, 0.9793244, 0.9793244),\n",
       " (149, 2, 0.29731375, 0.9834834, 0.9834834),\n",
       " (150, 0, 0.3056138, 1.0109391, 1.0109391),\n",
       " (150, 1, 0.3049995, 1.0089071, 1.0089071),\n",
       " (150, 2, 0.3038518, 1.0051106, 1.0051106),\n",
       " (151, 0, 0.30562654, 1.0109813, 1.0109813),\n",
       " (151, 1, 0.3015886, 0.99762416, 0.99762416),\n",
       " (151, 2, 0.3016741, 0.997907, 0.997907),\n",
       " (152, 0, 0.30309325, 1.0026014, 1.0026014),\n",
       " (152, 1, 0.30422458, 1.0063437, 1.0063437),\n",
       " (152, 2, 0.3028178, 1.0016901, 1.0016901),\n",
       " (153, 0, 0.30420455, 1.0062774, 1.0062774),\n",
       " (153, 1, 0.30453965, 1.007386, 1.007386),\n",
       " (153, 2, 0.30480218, 1.0082544, 1.0082544),\n",
       " (154, 0, 0.3049312, 1.0086812, 1.0086812),\n",
       " (154, 1, 0.30206984, 0.9992161, 0.9992161),\n",
       " (154, 2, 0.30003583, 0.9924878, 0.9924878),\n",
       " (155, 0, 0.30030712, 0.9933852, 0.9933852),\n",
       " (155, 1, 0.29921278, 0.9897652, 0.9897652),\n",
       " (155, 2, 0.2998521, 0.99188, 0.99188),\n",
       " (156, 0, 0.30507454, 1.0091553, 1.0091553),\n",
       " (156, 1, 0.30538616, 1.0101861, 1.0101861),\n",
       " (156, 2, 0.30297872, 1.0022225, 1.0022225),\n",
       " (157, 0, 0.30417192, 1.0061696, 1.0061696),\n",
       " (157, 1, 0.30434462, 1.0067408, 1.0067408),\n",
       " (157, 2, 0.30399284, 1.0055771, 1.0055771),\n",
       " (158, 0, 0.30395266, 1.0054443, 1.0054443),\n",
       " (158, 1, 0.30285463, 1.0018121, 1.0018121),\n",
       " (158, 2, 0.3031404, 1.0027573, 1.0027573),\n",
       " (159, 0, 0.30190307, 0.9986644, 0.9986644),\n",
       " (159, 1, 0.30177397, 0.9982373, 0.9982373),\n",
       " (159, 2, 0.30210817, 0.99934286, 0.99934286),\n",
       " (160, 0, 0.30227825, 0.99990547, 0.99990547),\n",
       " (160, 1, 0.30231416, 1.0000242, 1.0000242),\n",
       " (160, 2, 0.30344844, 1.0037763, 1.0037763),\n",
       " (161, 0, 0.30232242, 1.0000516, 1.0000516),\n",
       " (161, 1, 0.30262148, 1.0010408, 1.0010408),\n",
       " (161, 2, 0.30223382, 0.9997585, 0.9997585),\n",
       " (162, 0, 0.30111212, 0.99604803, 0.99604803),\n",
       " (162, 1, 0.30218232, 0.99958813, 0.99958813),\n",
       " (162, 2, 0.30152854, 0.9974255, 0.9974255),\n",
       " (163, 0, 0.30249253, 1.0006143, 1.0006143),\n",
       " (163, 1, 0.30230185, 0.99998355, 0.99998355),\n",
       " (163, 2, 0.30057582, 0.994274, 0.994274),\n",
       " (164, 0, 0.30221888, 0.99970907, 0.99970907),\n",
       " (164, 1, 0.30235144, 1.0001476, 1.0001476),\n",
       " (164, 2, 0.3022906, 0.9999463, 0.9999463),\n",
       " (165, 0, 0.30311474, 1.0026724, 1.0026724),\n",
       " (165, 1, 0.30186698, 0.998545, 0.998545),\n",
       " (165, 2, 0.30324277, 1.003096, 1.003096),\n",
       " (166, 0, 0.30212995, 0.9994149, 0.9994149),\n",
       " (166, 1, 0.30190173, 0.99865997, 0.99865997),\n",
       " (166, 2, 0.30216718, 0.99953806, 0.99953806),\n",
       " (167, 0, 0.30256525, 1.0008548, 1.0008548),\n",
       " (167, 1, 0.30206162, 0.99918884, 0.99918884),\n",
       " (167, 2, 0.30219808, 0.9996403, 0.9996403),\n",
       " (168, 0, 0.30433458, 1.0067075, 1.0067075),\n",
       " (168, 1, 0.30413195, 1.0060374, 1.0060374),\n",
       " (168, 2, 0.30456567, 1.007472, 1.007472),\n",
       " (169, 0, 0.30308753, 1.0025824, 1.0025824),\n",
       " (169, 1, 0.30191374, 0.99869967, 0.99869967),\n",
       " (169, 2, 0.30374894, 1.0047703, 1.0047703),\n",
       " (170, 0, 0.3039939, 1.0055807, 1.0055807),\n",
       " (170, 1, 0.3043184, 1.006654, 1.006654),\n",
       " (170, 2, 0.302634, 1.0010823, 1.0010823),\n",
       " (171, 0, 0.3034519, 1.0037878, 1.0037878),\n",
       " (171, 1, 0.30237848, 1.000237, 1.000237),\n",
       " (171, 2, 0.30117962, 0.9962713, 0.9962713),\n",
       " (172, 0, 0.30255684, 1.000827, 1.000827),\n",
       " (172, 1, 0.30242154, 1.0003794, 1.0003794),\n",
       " (172, 2, 0.302317, 1.0000336, 1.0000336),\n",
       " (173, 0, 0.30177194, 0.99823064, 0.99823064),\n",
       " (173, 1, 0.30187997, 0.99858797, 0.99858797),\n",
       " (173, 2, 0.3029437, 1.0021067, 1.0021067),\n",
       " (174, 0, 0.299687, 0.99133384, 0.99133384),\n",
       " (174, 1, 0.30235308, 1.000153, 1.000153),\n",
       " (174, 2, 0.3004665, 0.9939124, 0.9939124),\n",
       " (175, 0, 0.30235422, 1.0001568, 1.0001568),\n",
       " (175, 1, 0.3027483, 1.0014603, 1.0014603),\n",
       " (175, 2, 0.303341, 1.003421, 1.003421),\n",
       " (176, 0, 0.30230913, 1.0000076, 1.0000076),\n",
       " (176, 1, 0.3017179, 0.9980519, 0.9980519),\n",
       " (176, 2, 0.30223718, 0.9997696, 0.9997696),\n",
       " (177, 0, 0.30188438, 0.99860257, 0.99860257),\n",
       " (177, 1, 0.30271477, 1.0013494, 1.0013494),\n",
       " (177, 2, 0.30242512, 1.0003912, 1.0003912),\n",
       " (178, 0, 0.30152464, 0.99741256, 0.99741256),\n",
       " (178, 1, 0.3029221, 1.0020353, 1.0020353),\n",
       " (178, 2, 0.30153355, 0.99744207, 0.99744207),\n",
       " (179, 0, 0.30189818, 0.9986482, 0.9986482),\n",
       " (179, 1, 0.3001195, 0.9927645, 0.9927645),\n",
       " (179, 2, 0.30265152, 1.0011402, 1.0011402),\n",
       " (180, 0, 0.30116606, 0.99622643, 0.99622643),\n",
       " (180, 1, 0.30161896, 0.9977246, 0.9977246),\n",
       " (180, 2, 0.3017192, 0.9980561, 0.9980561),\n",
       " (181, 0, 0.29978415, 0.99165523, 0.99165523),\n",
       " (181, 1, 0.30171314, 0.99803615, 0.99803615),\n",
       " (181, 2, 0.30161604, 0.99771494, 0.99771494),\n",
       " (182, 0, 0.30139062, 0.9969693, 0.9969693),\n",
       " (182, 1, 0.30232686, 1.0000663, 1.0000663),\n",
       " (182, 2, 0.30166802, 0.9978869, 0.9978869),\n",
       " (183, 0, 0.302758, 1.0014925, 1.0014925),\n",
       " (183, 1, 0.30234143, 1.0001144, 1.0001144),\n",
       " (183, 2, 0.3027257, 1.0013856, 1.0013856),\n",
       " (184, 0, 0.30241033, 1.0003424, 1.0003424),\n",
       " (184, 1, 0.30260715, 1.0009934, 1.0009934),\n",
       " (184, 2, 0.3020582, 0.9991775, 0.9991775),\n",
       " (185, 0, 0.30206218, 0.99919075, 0.99919075),\n",
       " (185, 1, 0.30080876, 0.9950445, 0.9950445),\n",
       " (185, 2, 0.2953845, 0.9771016, 0.9771016),\n",
       " (186, 0, 0.29730925, 0.98346853, 0.98346853),\n",
       " (186, 1, 0.30210206, 0.99932265, 0.99932265),\n",
       " (186, 2, 0.30080435, 0.9950299, 0.9950299),\n",
       " (187, 0, 0.30133322, 0.9967794, 0.9967794),\n",
       " (187, 1, 0.30223185, 0.999752, 0.999752),\n",
       " (187, 2, 0.30034623, 0.99351454, 0.99351454),\n",
       " (188, 0, 0.3018267, 0.9984117, 0.9984117),\n",
       " (188, 1, 0.30196106, 0.99885625, 0.99885625),\n",
       " (188, 2, 0.30194852, 0.99881476, 0.99881476),\n",
       " (189, 0, 0.30241424, 1.0003552, 1.0003552),\n",
       " (189, 1, 0.30184692, 0.99847865, 0.99847865),\n",
       " (189, 2, 0.30239564, 1.0002937, 1.0002937),\n",
       " (190, 0, 0.30189005, 0.99862134, 0.99862134),\n",
       " (190, 1, 0.30253634, 1.0007592, 1.0007592),\n",
       " (190, 2, 0.30210927, 0.9993465, 0.9993465),\n",
       " (191, 0, 0.29729068, 0.9834071, 0.9834071),\n",
       " (191, 1, 0.30219764, 0.9996388, 0.9996388),\n",
       " (191, 2, 0.30227408, 0.99989164, 0.99989164),\n",
       " (192, 0, 0.30210814, 0.99934274, 0.99934274),\n",
       " (192, 1, 0.30277514, 1.0015491, 1.0015491),\n",
       " (192, 2, 0.3017638, 0.9982037, 0.9982037),\n",
       " (193, 0, 0.30152392, 0.99741024, 0.99741024),\n",
       " (193, 1, 0.30086112, 0.99521774, 0.99521774),\n",
       " (193, 2, 0.3013881, 0.996961, 0.996961),\n",
       " (194, 0, 0.3021508, 0.9994838, 0.9994838),\n",
       " (194, 1, 0.30262, 1.0010359, 1.0010359),\n",
       " (194, 2, 0.30262238, 1.0010438, 1.0010438),\n",
       " (195, 0, 0.30215016, 0.99948174, 0.99948174),\n",
       " (195, 1, 0.30321077, 1.0029901, 1.0029901),\n",
       " (195, 2, 0.30215663, 0.99950314, 0.99950314),\n",
       " (196, 0, 0.30243275, 1.0004165, 1.0004165),\n",
       " (196, 1, 0.3023141, 1.0000241, 1.0000241),\n",
       " (196, 2, 0.3026917, 1.0012732, 1.0012732),\n",
       " (197, 0, 0.30232355, 1.0000553, 1.0000553),\n",
       " (197, 1, 0.30208477, 0.99926543, 0.99926543),\n",
       " (197, 2, 0.30147514, 0.9972488, 0.9972488),\n",
       " (198, 0, 0.30144778, 0.99715835, 0.99715835),\n",
       " (198, 1, 0.3024064, 1.0003294, 1.0003294),\n",
       " (198, 2, 0.30165765, 0.99785256, 0.99785256),\n",
       " (199, 0, 0.30245286, 1.000483, 1.000483),\n",
       " (199, 1, 0.3022761, 0.9998984, 0.9998984),\n",
       " (199, 2, 0.30241978, 1.0003736, 1.0003736),\n",
       " (200, 0, 0.30229843, 0.9999722, 0.9999722),\n",
       " (200, 1, 0.3018499, 0.99848855, 0.99848855),\n",
       " (200, 2, 0.30178115, 0.9982611, 0.9982611),\n",
       " (201, 0, 0.29720104, 0.98311055, 0.98311055),\n",
       " (201, 1, 0.30370834, 1.004636, 1.004636),\n",
       " (201, 2, 0.30200204, 0.9989918, 0.9989918),\n",
       " (202, 0, 0.30283904, 1.0017605, 1.0017605),\n",
       " (202, 1, 0.30174276, 0.99813414, 0.99813414),\n",
       " (202, 2, 0.30207884, 0.9992458, 0.9992458),\n",
       " (203, 0, 0.3014489, 0.9971621, 0.9971621),\n",
       " (203, 1, 0.30178103, 0.9982607, 0.9982607),\n",
       " (203, 2, 0.30165982, 0.9978598, 0.9978598),\n",
       " (204, 0, 0.3006122, 0.99439436, 0.99439436),\n",
       " (204, 1, 0.3024672, 1.0005305, 1.0005305),\n",
       " (204, 2, 0.30192375, 0.9987328, 0.9987328),\n",
       " (205, 0, 0.3020335, 0.9990959, 0.9990959),\n",
       " (205, 1, 0.301497, 0.9973212, 0.9973212),\n",
       " (205, 2, 0.3015993, 0.99765956, 0.99765956),\n",
       " (206, 0, 0.30153012, 0.99743074, 0.99743074),\n",
       " (206, 1, 0.3015885, 0.99762386, 0.99762386),\n",
       " (206, 2, 0.30204263, 0.9991261, 0.9991261),\n",
       " (207, 0, 0.30064076, 0.99448884, 0.99448884),\n",
       " (207, 1, 0.3009835, 0.9956225, 0.9956225),\n",
       " (207, 2, 0.30070746, 0.99470943, 0.99470943),\n",
       " (208, 0, 0.30233794, 1.0001029, 1.0001029),\n",
       " (208, 1, 0.30254325, 1.000782, 1.000782),\n",
       " (208, 2, 0.30198315, 0.99892926, 0.99892926),\n",
       " (209, 0, 0.3019771, 0.9989093, 0.9989093),\n",
       " (209, 1, 0.30250287, 1.0006485, 1.0006485),\n",
       " (209, 2, 0.3024234, 1.0003855, 1.0003855),\n",
       " (210, 0, 0.3020106, 0.9990201, 0.9990201),\n",
       " (210, 1, 0.30220973, 0.9996788, 0.9996788),\n",
       " (210, 2, 0.30206156, 0.99918866, 0.99918866),\n",
       " (211, 0, 0.30267206, 1.0012082, 1.0012082),\n",
       " (211, 1, 0.302482, 1.0005795, 1.0005795),\n",
       " (211, 2, 0.3025004, 1.0006403, 1.0006403),\n",
       " (212, 0, 0.30217496, 0.99956375, 0.99956375),\n",
       " (212, 1, 0.3024017, 1.0003138, 1.0003138),\n",
       " (212, 2, 0.30245775, 1.0004992, 1.0004992),\n",
       " (213, 0, 0.3026407, 1.0011045, 1.0011045),\n",
       " (213, 1, 0.30266345, 1.0011797, 1.0011797),\n",
       " (213, 2, 0.30174306, 0.9981351, 0.9981351),\n",
       " (214, 0, 0.3029184, 1.002023, 1.002023),\n",
       " (214, 1, 0.30235752, 1.0001677, 1.0001677),\n",
       " (214, 2, 0.30304334, 1.0024363, 1.0024363),\n",
       " (215, 0, 0.30231538, 1.0000283, 1.0000283),\n",
       " (215, 1, 0.3023158, 1.0000297, 1.0000297),\n",
       " (215, 2, 0.30214247, 0.9994563, 0.9994563),\n",
       " (216, 0, 0.3031472, 1.0027798, 1.0027798),\n",
       " (216, 1, 0.3023594, 1.0001739, 1.0001739),\n",
       " (216, 2, 0.3028688, 1.001859, 1.001859),\n",
       " (217, 0, 0.30225343, 0.99982333, 0.99982333),\n",
       " (217, 1, 0.30241504, 1.000358, 1.000358),\n",
       " (217, 2, 0.30281806, 1.0016911, 1.0016911),\n",
       " (218, 0, 0.30251095, 1.0006752, 1.0006752),\n",
       " (218, 1, 0.30274925, 1.0014634, 1.0014634),\n",
       " (218, 2, 0.3009462, 0.9954992, 0.9954992),\n",
       " (219, 0, 0.2996486, 0.9912069, 0.9912069),\n",
       " (219, 1, 0.3031673, 1.0028464, 1.0028464),\n",
       " (219, 2, 0.30312023, 1.0026907, 1.0026907),\n",
       " (220, 0, 0.3030369, 1.002415, 1.002415),\n",
       " (220, 1, 0.30275574, 1.001485, 1.001485),\n",
       " (220, 2, 0.3026559, 1.0011547, 1.0011547),\n",
       " (221, 0, 0.30252093, 1.0007082, 1.0007082),\n",
       " (221, 1, 0.30261552, 1.0010211, 1.0010211),\n",
       " (221, 2, 0.30310202, 1.0026304, 1.0026304),\n",
       " (222, 0, 0.30295005, 1.0021278, 1.0021278),\n",
       " (222, 1, 0.3037303, 1.0047088, 1.0047088),\n",
       " (222, 2, 0.3030503, 1.0024594, 1.0024594),\n",
       " (223, 0, 0.30298266, 1.0022355, 1.0022355),\n",
       " (223, 1, 0.3032422, 1.0030941, 1.0030941),\n",
       " (223, 2, 0.30184722, 0.99847966, 0.99847966),\n",
       " (224, 0, 0.30292085, 1.0020311, 1.0020311),\n",
       " (224, 1, 0.30238166, 1.0002476, 1.0002476),\n",
       " (224, 2, 0.3002864, 0.99331665, 0.99331665),\n",
       " (225, 0, 0.3023709, 1.000212, 1.000212),\n",
       " (225, 1, 0.3027323, 1.0014074, 1.0014074),\n",
       " (225, 2, 0.30256516, 1.0008545, 1.0008545),\n",
       " (226, 0, 0.3022271, 0.9997363, 0.9997363),\n",
       " (226, 1, 0.302095, 0.9992993, 0.9992993),\n",
       " (226, 2, 0.3016394, 0.99779224, 0.99779224),\n",
       " (227, 0, 0.30239668, 1.0002972, 1.0002972),\n",
       " (227, 1, 0.3021423, 0.99945575, 0.99945575),\n",
       " (227, 2, 0.30212864, 0.99941057, 0.99941057),\n",
       " (228, 0, 0.30221766, 0.999705, 0.999705),\n",
       " (228, 1, 0.30229896, 0.99997395, 0.99997395),\n",
       " (228, 2, 0.3023286, 1.000072, 1.000072),\n",
       " (229, 0, 0.30243358, 1.0004193, 1.0004193),\n",
       " (229, 1, 0.30227014, 0.99987864, 0.99987864),\n",
       " (229, 2, 0.30287176, 1.0018687, 1.0018687),\n",
       " (230, 0, 0.29788086, 0.9853593, 0.9853593),\n",
       " (230, 1, 0.30261078, 1.0010054, 1.0010054),\n",
       " (230, 2, 0.3022617, 0.99985075, 0.99985075),\n",
       " (231, 0, 0.30236632, 1.0001968, 1.0001968),\n",
       " (231, 1, 0.30217656, 0.9995691, 0.9995691),\n",
       " (231, 2, 0.30212602, 0.99940187, 0.99940187),\n",
       " (232, 0, 0.30228528, 0.9999287, 0.9999287),\n",
       " (232, 1, 0.30238006, 1.0002422, 1.0002422),\n",
       " (232, 2, 0.3020931, 0.99929297, 0.99929297),\n",
       " (233, 0, 0.30222464, 0.9997281, 0.9997281),\n",
       " (233, 1, 0.30234, 1.0001097, 1.0001097),\n",
       " (233, 2, 0.3022197, 0.99971175, 0.99971175),\n",
       " (234, 0, 0.30267206, 1.0012082, 1.0012082),\n",
       " (234, 1, 0.3025919, 1.000943, 1.000943),\n",
       " (234, 2, 0.30266625, 1.0011889, 1.0011889),\n",
       " (235, 0, 0.30255654, 1.000826, 1.000826),\n",
       " (235, 1, 0.3026618, 1.0011742, 1.0011742),\n",
       " (235, 2, 0.30243418, 1.0004213, 1.0004213),\n",
       " (236, 0, 0.30218264, 0.9995892, 0.9995892),\n",
       " (236, 1, 0.30232298, 1.0000534, 1.0000534),\n",
       " (236, 2, 0.30211163, 0.9993543, 0.9993543),\n",
       " (237, 0, 0.30265647, 1.0011566, 1.0011566),\n",
       " (237, 1, 0.3024239, 1.0003872, 1.0003872),\n",
       " (237, 2, 0.30236375, 1.0001884, 1.0001884),\n",
       " (238, 0, 0.30245444, 1.0004883, 1.0004883),\n",
       " (238, 1, 0.30237937, 1.00024, 1.00024),\n",
       " (238, 2, 0.30231544, 1.0000285, 1.0000285),\n",
       " (239, 0, 0.3025347, 1.0007538, 1.0007538),\n",
       " (239, 1, 0.30271035, 1.0013348, 1.0013348),\n",
       " (239, 2, 0.30249557, 1.0006243, 1.0006243),\n",
       " (240, 0, 0.3022541, 0.9998256, 0.9998256),\n",
       " (240, 1, 0.3022114, 0.99968433, 0.99968433),\n",
       " (240, 2, 0.30233496, 1.0000931, 1.0000931),\n",
       " (241, 0, 0.30267978, 1.0012337, 1.0012337),\n",
       " (241, 1, 0.30266747, 1.0011929, 1.0011929),\n",
       " (241, 2, 0.30244288, 1.00045, 1.00045),\n",
       " (242, 0, 0.30235726, 1.0001668, 1.0001668),\n",
       " (242, 1, 0.3022202, 0.9997134, 0.9997134),\n",
       " (242, 2, 0.30237132, 1.0002134, 1.0002134),\n",
       " (243, 0, 0.3023312, 1.0000806, 1.0000806),\n",
       " (243, 1, 0.30260548, 1.0009879, 1.0009879),\n",
       " (243, 2, 0.30254516, 1.0007883, 1.0007883),\n",
       " (244, 0, 0.30227265, 0.99988693, 0.99988693),\n",
       " (244, 1, 0.3023454, 1.0001276, 1.0001276),\n",
       " (244, 2, 0.30238, 1.000242, 1.000242),\n",
       " (245, 0, 0.30227914, 0.9999084, 0.9999084),\n",
       " (245, 1, 0.30225942, 0.9998432, 0.9998432),\n",
       " (245, 2, 0.3021643, 0.99952847, 0.99952847),\n",
       " (246, 0, 0.30222845, 0.9997407, 0.9997407),\n",
       " (246, 1, 0.302296, 0.99996424, 0.99996424),\n",
       " (246, 2, 0.30222312, 0.9997231, 0.9997231),\n",
       " (247, 0, 0.30227676, 0.9999005, 0.9999005),\n",
       " (247, 1, 0.30221683, 0.9997023, 0.9997023),\n",
       " (247, 2, 0.30223563, 0.9997645, 0.9997645),\n",
       " (248, 0, 0.30224046, 0.9997805, 0.9997805),\n",
       " (248, 1, 0.30236617, 1.0001963, 1.0001963),\n",
       " (248, 2, 0.302366, 1.0001957, 1.0001957),\n",
       " (249, 0, 0.30230483, 0.9999934, 0.9999934),\n",
       " (249, 1, 0.3022198, 0.9997121, 0.9997121),\n",
       " (249, 2, 0.30223256, 0.9997543, 0.9997543)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HIS['HSALNT0068489']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf0da48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(list(lnc_fa.keys())[0:3])\n",
    "# random.seed(121)\n",
    "# dic1={\"A\": 0,\"T\": 1,\"G\": 2,\"C\": 3}\n",
    "# ATGC=list('ATGC')\n",
    "# HIS={}\n",
    "# LNC=[]\n",
    "# smut=[]\n",
    "# dmut=[]\n",
    "# def mutate(x,a,b):\n",
    "#     return x[:a]+ATGC[(dic1[x[a]]+b+1) % 4]+x[(a+1):]\n",
    "\n",
    "# for lnc in list(lnc_fa.keys())[0:3]:\n",
    "#     lnc_len=len(lnc_fa[lnc])\n",
    "#     mut1_num=min(250,lnc_len)\n",
    "#     his=[set([(-1,-1),(-1,-1)])]\n",
    "#     padded_lnc_list=[\"P\"*padding_size+lnc_fa[lnc]+\"P\"*padding_size]\n",
    "    \n",
    "#     for i in range(mut1_num):\n",
    "#         for j in range(3):\n",
    "#             his.append(set([(i,j),(-1,-1)]))\n",
    "#             padded_lnc_list.append(\"P\"*padding_size+mutate(lnc_fa[lnc],i,j)+\"P\"*padding_size)\n",
    "#             for k in range(50):\n",
    "#                 m=list(set(list(range(mut1_num)))-set([i]))[random.randint(0,mut1_num-2)]\n",
    "#                 n=random.randint(0,2)\n",
    "#                 while set([(i,j),(m,n)]) in his:\n",
    "#                     m=list(set(list(range(mut1_num)))-set([i]))[random.randint(0,mut1_num-2)]\n",
    "#                     n=random.randint(0,2)\n",
    "#                 his.append(set([(i,j),(m,n)]))\n",
    "#                 padded_lnc_list.append(\"P\"*padding_size+mutate(mutate(lnc_fa[lnc],i,j),m,n)+\"P\"*padding_size)\n",
    "#     my_seq = []\n",
    "#     my_target=[]\n",
    "#     my_weight=[]\n",
    "#     my_dis=[]\n",
    "#     for iii in range(len(padded_lnc_list)):\n",
    "#         ii=padded_lnc_list[iii]\n",
    "#         pattern=re.compile(r'(?=(ATG|TTG|CTG|GTG|AAG|ACG|AGG|ATA|ATT|ATC))')\n",
    "#         it=re.finditer(pattern,ii)\n",
    "#         pos3=[i.span()[0] for i in it][0:(TIS_max_num+3)]\n",
    "#         pos2=pos3[0:(TIS_max_num+1)]\n",
    "#         pos=pos2[0:TIS_max_num]\n",
    "#         if len(pos2)==len(pos):\n",
    "#             pos2=pos2+[pos2[len(pos2)-1]+20]\n",
    "#         my_seq.append(torch.stack([torch.tensor([dict[j] for j in list(ii[(p-padding_size):(p+padding_size+1)])], dtype=torch.long) for p in pos]))\n",
    "#         my_target.append(torch.tensor([1 for j in range(len(pos))]))\n",
    "#         my_weight.append(torch.tensor([1]*len(pos)))\n",
    "#         pos1=[padding_size]+pos\n",
    "#         my_dis.append(torch.stack([torch.tensor([pos1[i+1]-pos1[i] for i in range(len(pos1)-1)]),torch.tensor([pos2[i+1]-pos2[i] for i in range(len(pos2)-1)])],1))\n",
    "#     my_dis=[i/sqrt_scale for i in my_dis]\n",
    "#     MyvalData = testoneMyDataset(inputset=my_seq, targetset=my_target, weightset=my_weight, disset=my_dis, transcriptset=his)\n",
    "#     Myvalloader = torch.utils.data.DataLoader(MyvalData, batch_size=len(MyvalData), shuffle=False, num_workers=0, collate_fn=collate_fn_padd, drop_last=True)\n",
    "#     iter0=iter(enumerate(Myvalloader, 0))\n",
    "#     iii, batch=next(iter0)\n",
    "#     inputs, sizes, labels, weights, diss, transcriptt = batch\n",
    "#     o1=Mymodel1(x=inputs, sizes=sizes, diss=diss)\n",
    "#     o2=Mymodel2(x=inputs, sizes=sizes, diss=diss)\n",
    "#     o3=Mymodel3(x=inputs, sizes=sizes, diss=diss)\n",
    "#     o4=Mymodel4(x=inputs, sizes=sizes, diss=diss)\n",
    "#     o5=Mymodel5(x=inputs, sizes=sizes, diss=diss)\n",
    "#     o6=Mymodel6(x=inputs, sizes=sizes, diss=diss)\n",
    "#     o7=Mymodel7(x=inputs, sizes=sizes, diss=diss)\n",
    "#     o8=Mymodel8(x=inputs, sizes=sizes, diss=diss)\n",
    "#     o9=Mymodel9(x=inputs, sizes=sizes, diss=diss)\n",
    "#     o10=Mymodel10(x=inputs, sizes=sizes, diss=diss)\n",
    "#     outputs=torch.stack((o1,o2,o3,o4,o5,o6,o7,o8,o9,o10))\n",
    "#     del o1,o2,o3,o4,o5,o6,o7,o8,o9,o10\n",
    "    \n",
    "#     outputs=torch.mean(outputs/torch.sum(torch.abs(outputs),2,keepdim=True),0,keepdim=False)\n",
    "#     max_peak,indices = torch.max(outputs,1,keepdim=False)\n",
    "#     outputs=max_peak/outputs[0][indices]\n",
    "    \n",
    "#     LNC.append(lnc)\n",
    "#     smut.append(np.array(torch.mean(outputs[list(range(1,len(outputs),51))]).cpu()))\n",
    "#     dmut.append(np.array(torch.mean(outputs[list(set(range(len(outputs)))-set([0]+list(range(1,len(outputs),51))))]).cpu()))\n",
    "    \n",
    "#     max_peak=np.array(max_peak.cpu())\n",
    "#     outputs=np.array(outputs.cpu())\n",
    "#     HIS[lnc]=[(list(i)[0][0],list(i)[0][1],list(i)[1][0],list(i)[1][1],outputs[i],max_peak[i]) for i in range(len(his))]\n",
    "#     #pd.DataFrame([[list(i)[0][0],list(i)[0][1],list(i)[1][0],list(i)[1][1]] for i in his]).to_csv(\"test.txt\",sep=\"\\t\")\n",
    "# pd.DataFrame({\"lncRNA\": LNC,\"smut\": smut,\"dmut\": dmut}).to_csv(\"test.txt\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71adaee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
